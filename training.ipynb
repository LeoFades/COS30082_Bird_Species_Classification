{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap6QjF-8M2WB"
      },
      "source": [
        "COS30082 Applied Machine Learning - Assignment 1 - Bird Species Classification\n",
        "\n",
        "Done by Leonardo Liew 102781996"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTu1WBaIMKop"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNAySsvbQL6O"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SatJ9VU6NSZr"
      },
      "outputs": [],
      "source": [
        "#### Configurations\n",
        "\n",
        "\n",
        "# Update Config to point to your Google Drive\n",
        "class Config:\n",
        "    # Paths - UPDATED FOR GOOGLE DRIVE\n",
        "    # Change \"CUB200\" to match YOUR folder name in Drive!\n",
        "    TRAIN_DIR = '/content/drive/MyDrive/AML/Train'\n",
        "    TEST_DIR = '/content/drive/MyDrive/AML/Test'\n",
        "    TRAIN_LABELS = '/content/drive/MyDrive/AML/train.txt'\n",
        "    TEST_LABELS = '/content/drive/MyDrive/AML/test.txt'\n",
        "\n",
        "    # Model parameters\n",
        "    NUM_CLASSES = 200\n",
        "    IMG_SIZE = 224\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_EPOCHS = 40\n",
        "\n",
        "    # Training hyperparameters\n",
        "    LEARNING_RATE = 0.0001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Model choice\n",
        "    MODEL_NAME = 'convnext_tiny'\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Verify paths exist\n",
        "import os\n",
        "print(\"✓ Checking paths...\")\n",
        "print(f\"Train folder exists: {os.path.exists(config.TRAIN_DIR)}\")\n",
        "print(f\"Test folder exists: {os.path.exists(config.TEST_DIR)}\")\n",
        "print(f\"train.txt exists: {os.path.exists(config.TRAIN_LABELS)}\")\n",
        "print(f\"test.txt exists: {os.path.exists(config.TEST_LABELS)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04sRK4C2NYAi"
      },
      "outputs": [],
      "source": [
        "#### Dataset class\n",
        "class BirdDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for CUB-200 birds\"\"\"\n",
        "\n",
        "    def __init__(self, img_dir, label_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir: Directory with all images\n",
        "            label_file: Path to txt file with annotations\n",
        "            transform: Optional transform to be applied on images\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "\n",
        "        # Read annotations\n",
        "        with open(label_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 2:\n",
        "                    img_name, label = parts[0], int(parts[1])\n",
        "                    self.data.append((img_name, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, label = self.data[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdyC1Q-WNexy"
      },
      "outputs": [],
      "source": [
        "#### data augmentation and transformations\n",
        "\n",
        "def get_transforms():\n",
        "    \"\"\"\n",
        "    Returns train and test transforms\n",
        "    Train: Heavy augmentation to prevent overfitting\n",
        "    Test: Only resize and normalize (no augmentation!)\n",
        "    \"\"\"\n",
        "\n",
        "    # Training transforms\n",
        "    train_transform = transforms.Compose([\n",
        "\n",
        "        ##### Light Augmentation ####\n",
        "        # transforms.RandomResizedCrop(config.IMG_SIZE, scale=(0.9, 1.0)),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        # transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "\n",
        "        ###### Medium augmentation #####\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "        transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomResizedCrop(config.IMG_SIZE, scale=(0.8, 1.0)),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.Resize((256, 256)),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                           [0.229, 0.224, 0.225])  # ImageNet stats\n",
        "    ])\n",
        "\n",
        "    # Test transforms - NO AUGMENTATION (only preprocessing)\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                           [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, test_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBnXg-vDNqm6"
      },
      "outputs": [],
      "source": [
        "### model building\n",
        "## the model building here is made so that a change of name\n",
        "## is all that is required for the change of model.\n",
        "\n",
        "def build_model(model_name='convnext_tiny', num_classes=200, pretrained=True):\n",
        "    \"\"\"\n",
        "    Build transfer learning model\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of pretrained model\n",
        "        num_classes: Number of output classes\n",
        "        pretrained: Whether to use pretrained weights\n",
        "\n",
        "    Returns:\n",
        "        model: PyTorch model\n",
        "    \"\"\"\n",
        "\n",
        "    if model_name == 'resnet50':\n",
        "        model = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Freeze all layers initially\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace final layer with dropout\n",
        "        num_features = model.fc.in_features\n",
        "\n",
        "\n",
        "        # model.fc = nn.Sequential(\n",
        "        #     nn.Dropout(0.5),  # Dropout for regularization\n",
        "        #     nn.Linear(num_features, num_classes)\n",
        "        # )\n",
        "\n",
        "        model.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(num_features, 512),\n",
        "          nn.BatchNorm1d(512),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Dropout(0.5),\n",
        "          nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # this freeze backbone initially\n",
        "        # Freeze to train classifier head\n",
        "        for param in model.parameters():\n",
        "          param.requires_grad = False\n",
        "        for param in model.fc.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "\n",
        "    elif model_name == 'convnext_tiny':\n",
        "      model = models.convnext_tiny(pretrained=pretrained)\n",
        "\n",
        "      # Freeze backbone\n",
        "      for param in model.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "      # Replace the classifier head\n",
        "      num_features = model.classifier[2].in_features\n",
        "      model.classifier[2] = nn.Linear(num_features, num_classes)\n",
        "\n",
        "      # Add dropout for regularization\n",
        "      # model.classifier = nn.Sequential(\n",
        "      #     nn.Flatten(),\n",
        "      #     nn.Dropout(0.5),\n",
        "      #     nn.Linear(num_features, num_classes)\n",
        "      # )\n",
        "\n",
        "      model.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(num_features, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(512, num_classes)\n",
        "      )\n",
        "\n",
        "      # Unfreeze only the classifier first\n",
        "      for param in model.classifier.parameters():\n",
        "          param.requires_grad = True\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not supported\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdhcExzeN2Sq"
      },
      "outputs": [],
      "source": [
        "### training n validation functions\n",
        "## this trains one epoch for the training loop\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'loss': loss.item(),\n",
        "                         'acc': 100. * correct / total})\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc='Validation'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc, np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "def calculate_per_class_accuracy(predictions, labels, num_classes=200):\n",
        "    \"\"\"Calculate average accuracy per class\"\"\"\n",
        "    class_correct = np.zeros(num_classes)\n",
        "    class_total = np.zeros(num_classes)\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        class_total[label] += 1\n",
        "        if pred == label:\n",
        "            class_correct[label] += 1\n",
        "\n",
        "    # Avoid division by zero\n",
        "    class_acc = np.divide(class_correct, class_total,\n",
        "                          out=np.zeros_like(class_correct),\n",
        "                          where=class_total!=0)\n",
        "\n",
        "    avg_class_acc = np.mean(class_acc[class_total > 0])\n",
        "    return avg_class_acc * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXmOTl-jOHWA"
      },
      "outputs": [],
      "source": [
        "## early stopping\n",
        "# this is to stop if it is overfitting\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "\n",
        "    def __init__(self, patience=7, min_delta=0, mode='min'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.mode = mode\n",
        "\n",
        "    def __call__(self, score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif self._is_improvement(score):\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def _is_improvement(self, score):\n",
        "        if self.mode == 'min':\n",
        "            return score < self.best_score - self.min_delta\n",
        "        else:  # mode == 'max'\n",
        "            return score > self.best_score + self.min_delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmp1cMAEOKKK"
      },
      "outputs": [],
      "source": [
        "### main training loop\n",
        "## there are two phase to training loop\n",
        "## first - frozen backbone\n",
        "## second - unfreeze backbone\n",
        "\n",
        "def main():\n",
        "    print(f\"Using device: {config.DEVICE}\")\n",
        "    print(f\"Model: {config.MODEL_NAME}\")\n",
        "\n",
        "    # Get transforms\n",
        "    train_transform, test_transform = get_transforms()\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = BirdDataset(config.TRAIN_DIR, config.TRAIN_LABELS,\n",
        "                                train_transform)\n",
        "    test_dataset = BirdDataset(config.TEST_DIR, config.TEST_LABELS,\n",
        "                               test_transform)\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Testing samples: {len(test_dataset)}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n",
        "                            shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(config.MODEL_NAME, config.NUM_CLASSES)\n",
        "    model = model.to(config.DEVICE)\n",
        "\n",
        "    # Loss function with label smoothing (helps with overfitting)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # Optimizer with weight decay (L2 regularization)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                          lr=config.LEARNING_RATE,\n",
        "                          weight_decay=config.WEIGHT_DECAY)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
        "\n",
        "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "    #                                                  factor=0.5, patience=3)\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(patience=7, mode='min')\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_avg_class_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    # PHASE 1: Train only classifier (frozen backbone)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PHASE 1: Training classifier only (frozen backbone)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader,\n",
        "                                                criterion, optimizer,\n",
        "                                                config.DEVICE)\n",
        "\n",
        "        # Validate (monitoring test set, NOT training on it)\n",
        "        val_loss, val_acc, preds, labels = validate(model, test_loader,\n",
        "                                                     criterion, config.DEVICE)\n",
        "\n",
        "        # Calculate per-class accuracy\n",
        "        avg_class_acc = calculate_per_class_accuracy(preds, labels)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_avg_class_acc'].append(avg_class_acc)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"Val Avg Class Acc: {avg_class_acc:.2f}%\")\n",
        "        print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'avg_class_acc': avg_class_acc,\n",
        "            }, 'best_model.pth')\n",
        "            print(f\"✓ Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"\\n⚠ Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "\n",
        "        # === Phase 2 trigger ===\n",
        "        if epoch == 5:  # after 5 epochs of warmup\n",
        "            print(\"🔓 Unfreezing entire model for fine-tuning...\")\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = True\n",
        "            # for param in model.layer1.parameters():\n",
        "            #     param.requires_grad = False\n",
        "            # for param in model.layer2.parameters():\n",
        "            #     param.requires_grad = False\n",
        "\n",
        "\n",
        "            optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                                          lr=config.LEARNING_RATE * 0.1,\n",
        "                                          weight_decay=config.WEIGHT_DECAY * 10)\n",
        "            #### Dont use this step thing cuz too abrupt\n",
        "            # scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "            #                                             step_size=10,\n",
        "            #                                             gamma=0.1)\n",
        "\n",
        "\n",
        "            # optimizer = torch.optim.AdamW([\n",
        "            #   # {'params': model.layer1.parameters(), 'lr': 1e-5},\n",
        "            #   # {'params': model.layer2.parameters(), 'lr': 1e-5},\n",
        "            #   {'params': model.layer3.parameters(), 'lr': 3e-5},\n",
        "            #   {'params': model.layer4.parameters(), 'lr': 1e-5},\n",
        "            #   {'params': model.fc.parameters(), 'lr': 1e-4},\n",
        "            # ], weight_decay=5e-4)\n",
        "\n",
        "            # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "              optimizer,\n",
        "              mode='min',\n",
        "              factor=0.5,\n",
        "              patience=2\n",
        "            )\n",
        "\n",
        "    # OPTIONAL PHASE 2: Fine-tune last layers\n",
        "    # Uncomment if you want to unfreeze and fine-tune\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PHASE 2: Fine-tuning last layers\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Unfreeze last layers\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"layer4\" in name or \"fc\" in name:  # For ResNet\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Lower learning rate for fine-tuning\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                          lr=config.LEARNING_RATE * 0.1,\n",
        "                          weight_decay=config.WEIGHT_DECAY)\n",
        "\n",
        "    # Continue training...\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSPZkroDOf5Y"
      },
      "outputs": [],
      "source": [
        "###visualisation\n",
        "## this plots out the loss n accuracy graphs\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training curves\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Accuracy\n",
        "    axes[1].plot(history['train_acc'], label='Train Acc')\n",
        "    axes[1].plot(history['val_acc'], label='Val Top-1 Acc')\n",
        "    axes[1].plot(history['val_avg_class_acc'], label='Val Avg Class Acc')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy (%)')\n",
        "    axes[1].set_title('Training and Validation Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FJoclXUOiTE"
      },
      "outputs": [],
      "source": [
        "## this just runs everything\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, history = main()\n",
        "    plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJExD9Wopw8l"
      },
      "outputs": [],
      "source": [
        "## copies the training history graph from runtime to drive folder\n",
        "\n",
        "!cp /content/training_history.png /content/drive/MyDrive/AML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CsgeNiKsjNZ"
      },
      "outputs": [],
      "source": [
        "## copies the model from runtime to drive folder\n",
        "\n",
        "!cp /content/best_model.pth /content/drive/MyDrive/AML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint training below"
      ],
      "metadata": {
        "id": "4McxNA1dyPpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## this is checkpoint training for the model\n",
        "# because there wasnt enough free units on google colab for one whole training\n",
        "# also because the accuracy graph looks like it can go higher\n",
        "# as it has not plateaued\n",
        "\n",
        "# === Load checkpoint ===\n",
        "checkpoint = torch.load('/content/best_model.pth', map_location='cuda', weights_only=False)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                                          lr=config.LEARNING_RATE * 0.1,\n",
        "                                          weight_decay=config.WEIGHT_DECAY * 10)\n",
        "train_transform, test_transform = get_transforms()\n",
        "\n",
        "train_dataset = BirdDataset(config.TRAIN_DIR, config.TRAIN_LABELS,\n",
        "                                train_transform)\n",
        "test_dataset = BirdDataset(config.TEST_DIR, config.TEST_LABELS,\n",
        "                               test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n",
        "                            shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# restore model and optimizer states\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# continue from where you left off\n",
        "start_epoch = checkpoint['epoch'] + 1\n",
        "best_val_acc = checkpoint['val_acc']\n",
        "\n",
        "print(f\"Resuming training from epoch {start_epoch}, best val acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "# === Optionally lower learning rate for fine-tuning ===\n",
        "for g in optimizer.param_groups:\n",
        "    g['lr'] *= 0.1  # reduce LR by 10x to fine-tune\n",
        "\n",
        "# === Continue training ===\n",
        "num_additional_epochs = 10  # or however many you want\n",
        "for epoch in range(start_epoch, start_epoch + num_additional_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, config.DEVICE)\n",
        "    val_loss, val_acc, preds, labels = validate(model, test_loader, criterion, config.DEVICE)\n",
        "    avg_class_acc = calculate_per_class_accuracy(preds, labels)\n",
        "\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_avg_class_acc'].append(avg_class_acc)\n",
        "\n",
        "    # check if improved\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'avg_class_acc': avg_class_acc,\n",
        "        }, 'best_model.pth')\n",
        "        print(f\"✓ Saved new best model (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}]  Train Acc: {train_acc:.2f}%  Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "ylqdKy7nySEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "below is more visualisation"
      ],
      "metadata": {
        "id": "GDOTHnG_8Bha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## here are more visualisations done at the end\n",
        "## this is to prep the visualisation variables for the function\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_all_preds(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Collecting predictions\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)"
      ],
      "metadata": {
        "id": "cwiPed-w72E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds, labels = get_all_preds(model, test_loader, config.DEVICE)\n"
      ],
      "metadata": {
        "id": "1SdPqQ9i74wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## top 20 worst performing class\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "num_classes = config.NUM_CLASSES\n",
        "cm = confusion_matrix(labels, preds)\n",
        "class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "\n",
        "# Handle any divisions by zero (if a class never appeared)\n",
        "class_acc = np.nan_to_num(class_acc)\n",
        "\n",
        "# Get indices of 20 worst-performing classes\n",
        "worst_indices = np.argsort(class_acc)[:20]\n",
        "print(\"🔍 20 Worst Classes:\")\n",
        "for idx in worst_indices:\n",
        "    print(f\"{idx}: {class_acc[idx]*100:.2f}% accuracy\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tU89Lzvm7_pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load class names\n",
        "\n",
        "def load_class_names(label_file):\n",
        "    \"\"\"\n",
        "    Load class names from your train.txt or test.txt file.\n",
        "    Example line: Black_footed_Albatross_0033_2472109427.jpg 0\n",
        "    \"\"\"\n",
        "    id_to_class = {}\n",
        "    with open(label_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                filename, class_id = parts\n",
        "                # Extract species name part only (everything before last two underscores)\n",
        "                class_name = \"_\".join(filename.split(\"_\")[:-2])\n",
        "                id_to_class[int(class_id)] = class_name.replace(\"_\", \" \")\n",
        "    return [id_to_class[i] for i in sorted(id_to_class.keys())]\n",
        "class_names = load_class_names(config.TRAIN_LABELS)\n",
        "\n"
      ],
      "metadata": {
        "id": "NPRHUJbF9FFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## confusion matrix of top 20 worst classes\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# === CONFIG ===\n",
        "num_classes = 200\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === 1. Recreate Model Architecture (ConvNeXt-Tiny in your case) ===\n",
        "model = models.convnext_tiny(weights=None)\n",
        "num_features = model.classifier[2].in_features  # 768\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.BatchNorm1d(512),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, num_classes)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# === 2. Load Checkpoint ===\n",
        "checkpoint = torch.load('/content/best_model.pth', map_location=device, weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(\"✅ Model loaded successfully.\")\n",
        "\n",
        "# === 3. Load Dataset (Test Set) ===\n",
        "# Make sure you have these functions from your original training script\n",
        "train_transform, test_transform = get_transforms()\n",
        "\n",
        "test_dataset = BirdDataset(config.TEST_DIR, config.TEST_LABELS, test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE,\n",
        "                         shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# === 4. Collect Predictions ===\n",
        "def get_all_preds(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Collecting predictions\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "all_preds, all_labels = get_all_preds(model, test_loader, device)\n",
        "\n",
        "# === 5. Compute Confusion Matrix ===\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# === 6. Identify 20 Worst Performing Classes ===\n",
        "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "worst_indices = np.argsort(class_accuracy)[:20]\n",
        "\n",
        "# assuming class_names is already defined\n",
        "cm_worst = cm[np.ix_(worst_indices, worst_indices)]\n",
        "worst_class_names = [class_names[i] for i in worst_indices]\n",
        "\n",
        "# === 7. Plot Confusion Matrix ===\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_worst,\n",
        "                              display_labels=worst_class_names)\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp.plot(cmap='Reds', xticks_rotation=90, colorbar=True, ax=ax)\n",
        "ax.set_title(\"Confusion Matrix — 20 Worst Performing Classes\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8NivprCeWkmV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}